# AI-Generated Text Detection using BERT :smiley::star2:

AI-Generated Text detection using BERT( Bi-directional Encoder Representation Transformer) is from the family of LLMs, which has been used for classification of human-authored texts and AI-Generated Texts. It uses various feature selection techniques in core which is used for categorising the texts generated by the two groups. It detects the two based on the semantic differencies, usage of vocabulary, statistical distributions and sentiment analysis measures. This method of detection comes in the family of Black-Box Detection Algorithms for AI-Text Detection. 

# Table of Contents :bulb:

- Introduction
- Work Flow
- Comprehensive explanation on how does BERT detect AI-Generated texts
- Youtube Video Explanation
- Edge Cases
- Tips & Follow Through

# Introduction :maple_leaf:

AI-generated content is becoming increasingly sophisticated, making it challenging to distinguish between genuine and computer-generated text. Fraud emails and Fake news are becoming everyday's story.My project aims to tackle this issue by leveraging the power of BERT (Bidirectional Encoder Representations from Transformers) to identify and flag AI-generated text segments. This would allow the AI to advance at much faster pace but not at the expense of human security and fundamental integrity.

# Work Flow :snowflake:

The solution follows a comprehensive approach to detect AI-generated texts :bulb:

- **Data Preprocessing:** We clean and preprocess the textual data, removing the noise, stop words, puntuations and non-alphabatical words. This was done using Bert-Preprocess

- **Additional Datasets** I collected various datasets which were hosted by various competitions like the one in analysis, and concatinated them and used the collective data to train my model. This increased my training instances form 1378 to around 50k. Allowing my model to train on a large set of varrying data allowing effective feature identification and implementation.

- **Model Training:** Using a BERT-based sequence classification model, we train the system to distinguish between genuine and AI-generated text with a high degree of accuracy. Here I used bert-base-uncase, one may also use bert-large-cased but that may significantly increase the training time as it has 24 encoder layers instead of 12 in the case of base version, which would reduce the submission score.

- **Predictions:** Once trained, the model generates predictions for test data, highlighting potential AI-generated content segments. The predictions have been done upon the test data.

- **Result Analysis:** The results are then saved in a CSV file, which was then submitted in the contest.

# Comprehensive explanation on how does BERT detect AI-Generated texts :hourglass_flowing_sand:

An extensive analysis was done by me on how does BERT detect AI-Generated texts and what are the low lying features bert capture to segregate the texts. For this task, I read various research papers on blogs to understand various aspects in and out of the range of the problem statement, allowing me to get a better grip upon the underlying mechnaism responsible. I have put all the research papers and the blogs I used for this purpose in the folder.

## The analysis of mine answers various questions like:

- What is black-box and white-box detection ? | And which one of the both is the problem statement ?
- What is watermark embeddings and how are they used by white-box detectors?
- Why detecting generated texts is important ?
- What is LLM hallucinations ? | And how can it be prevented ?
- What featured does bert use to identifiy b/w the generated texts and human authored texts ?
- How we do structure the pipeline of solving this problem ?
- What are the possible edge cases and the explanations for them ?

![image](https://github.com/beingamanforever/LLM-Text-Detection/assets/121532863/974fcdab-8375-47d1-b9e6-c1925883161c)

![image](https://github.com/beingamanforever/LLM-Text-Detection/assets/121532863/37ab40db-c6a2-4b33-ba36-3dcbbff27094)

![image](https://github.com/beingamanforever/LLM-Text-Detection/assets/121532863/01b881b6-58aa-449b-bbb0-679c313969b5)

Above are the instances from the analysis I done :anchor:

> [!NOTE]
> :bulb: The complete PDF analysis of mine can be accessed [here](https://drive.google.com/file/d/18G5C3skP-vLt-l6ihj83CP1ZoNbguXQc/view?usp=drive_link) :saxophone:

# Youtube Video Explanation :cactus:

I did a complete video explaining the analysis I had done, as words speak more than text. It would give a better understanding of how I structured the pipeline, and what exactly was my thought process during the course of action. The videos can be accessed here 

[<img src="https://plus.unsplash.com/premium_photo-1701094771979-e4042ee524d0?w=500&auto=format&fit=crop&q=60&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxlZGl0b3JpYWwtZmVlZHwxN3x8fGVufDB8fHx8fA%3D%3D" width="600" height="300"
/>](https://youtu.be/F2Zj1uHo0hs?si=pxLdbzHyCkYCXY5i)

[<img src="https://plus.unsplash.com/premium_photo-1670106461988-039155ad1cbf?w=500&auto=format&fit=crop&q=60&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxlZGl0b3JpYWwtZmVlZHwyN3x8fGVufDB8fHx8fA%3D%3D" width="600" height="300"
/>](https://youtu.be/kUS1GSaEm10?si=qNCRy-AgkYh8QVKv)

> [!NOTE]
> :bulb: Click on the images above and they would route you towards the video. Do hit a like if u liked my explanation and share it if possible hehe (usual youtuber line hehe).

# Edge Cases :bulb:
![possible_edge_case (1)](https://github.com/beingamanforever/LLM-Text-Detection/assets/121532863/f974b601-5893-41ab-b832-2a9e70cb6295)

> [!NOTE]
> :bulb: A few edge cases like the one above and possible explanations of it have been included in the Edge-Cases folder hosted by this repository.

It points out few of the underlying assumptions and details that have not been provided in the problem statement. Suitable logical arguments regarding the same have also been attached.

# Tips & Follow Through :sunflower:

> [!TIP]
> Below are a tips I would like to share :bulb:

- Using bert-large instead of bert-base may cause time limit error although the depth of analysis would be more
- Do make sure while running the notebook you have the exact path of the kaggle datasets, orelse it would throw submission error.
- Although the additional datasets don't require their exact paths to be present, you can just download them locally and then upload them to start the analysis.
- using bert pre-process instead of the usual nlp pipeline of preprocessing using nltk would fetch better results as because its better trained.
- we can train on even more data for better analysis, keeping in mind that the time-limit was 9hrs and my model took around 2.5hrs suggests we can play with more datasets, some good data compilations I found apart from the ones I used were as follows - [Aa](https://www.kaggle.com/datasets/radek1/llm-generated-essays), [Bb](https://www.kaggle.com/competitions/llm-detect-ai-generated-text/discussion/452127), [Cc](https://www.kaggle.com/competitions/llm-detect-ai-generated-text/discussion/452464)

## Variations and expanding the depth of analysis: :seedling:

- One may do fine-tuning for optimizing various hyper-parameters involved, like number of epochs, optimizer being used, batch size, number of layers, varrying the activation function of the bert layer.
- I did few of them and here is what I found, decreasing the batch size increased the accuracy in offline run but the submission accuracy in the contest somehow decreased.
- Adam did a far better job than compared to RMSE, but one may experiment with SGD as it generalizes well at an expense of accuracy.

## Author
[Aman Behera](https://github.com/beingamanforever) , IIT Roorkee - if u have any suggestions do let me know at aman_b@ch.iitr.ac.in :hamster:
